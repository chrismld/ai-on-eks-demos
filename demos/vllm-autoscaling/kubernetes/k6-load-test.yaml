apiVersion: v1
kind: ConfigMap
metadata:
  name: k6-script
  namespace: default
data:
  load-test.js: |
    import http from 'k6/http';
    import { check, sleep } from 'k6';

    // Staged Load Test for vLLM Autoscaling with GPU Node Provisioning
    //
    // Total duration: 30 minutes
    //
    // Phase 1 (0-30s):      Light load (5â†’10 VUs) - trigger initial scale-up
    // Phase 2 (30s-5m):     Steady light load (15 VUs) - wait for first nodes
    // Phase 3 (5m-5m30s):   Ramp to medium (50 VUs) - trigger more scaling
    // Phase 4 (5m30s-10m):  Hold medium load - wait for all nodes ready
    // Phase 5 (10m-11m):    Ramp to high load (150 VUs) - full capacity test
    // Phase 6 (11m-28m):    Sustained high load - demonstrate scaled system (17 mins)
    // Phase 7 (28m-30m):    Ramp down
    //
    // With 10 pods at ~30 RPS each, expect ~300 RPS at full scale

    export const options = {
      scenarios: {
        staged_scaling: {
          executor: 'ramping-vus',
          startVUs: 5,
          gracefulStop: '60s',
          stages: [
            // Phase 1: Light load to trigger initial scaling
            { duration: '30s', target: 10 },
            // Phase 2: Hold light load while first GPU nodes provision (~5 min)
            { duration: '4m30s', target: 15 },
            // Phase 3: Ramp to medium load to trigger more scaling
            { duration: '30s', target: 50 },
            // Phase 4: Hold medium load while remaining nodes provision (~4.5 min)
            { duration: '4m30s', target: 50 },
            // Phase 5: Ramp to high load - all pods should be ready
            { duration: '1m', target: 150 },
            // Phase 6: Sustained high load - demonstrate full capacity (17 mins)
            { duration: '17m', target: 150 },
            // Phase 7: Ramp down
            { duration: '2m', target: 0 },
          ],
        },
      },
      thresholds: {
        http_req_duration: ['p(95)<20000'], // 95% under 20s (allow for cold starts)
        http_req_failed: ['rate<0.15'],     // Allow 15% failures during scaling
      },
    };

    const prompts = [
      'Explain Kubernetes autoscaling in one sentence.',
      'What is KEDA and how does it work?',
      'Describe GPU inference optimization briefly.',
      'What is Karpenter used for?',
      'Explain horizontal pod autoscaling.',
    ];

    export default function () {
      const url = 'http://vllm-service.default.svc.cluster.local:8000/v1/completions';
      const prompt = prompts[Math.floor(Math.random() * prompts.length)];
      
      const payload = JSON.stringify({
        model: 'TheBloke/Mistral-7B-Instruct-v0.2-AWQ',
        prompt: prompt,
        max_tokens: 50,
        temperature: 0.7,
      });

      const params = {
        headers: { 'Content-Type': 'application/json' },
        timeout: '45s',
      };

      const res = http.post(url, payload, params);
      
      check(res, {
        'status is 200': (r) => r.status === 200,
        'has completion': (r) => {
          try {
            const body = JSON.parse(r.body);
            return body.choices && body.choices.length > 0;
          } catch (e) {
            return false;
          }
        },
      });

      // Small sleep to control request rate per VU
      sleep(0.2 + Math.random() * 0.3);
    }
---
apiVersion: batch/v1
kind: Job
metadata:
  name: k6-load-test
  namespace: default
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 600
  template:
    spec:
      terminationGracePeriodSeconds: 120
      restartPolicy: Never
      containers:
      - name: k6
        image: grafana/k6:latest
        command: ["k6", "run", "--out", "json=/tmp/results.json", "/scripts/load-test.js"]
        volumeMounts:
        - name: k6-script
          mountPath: /scripts
        resources:
          requests:
            cpu: 500m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 512Mi
      volumes:
      - name: k6-script
        configMap:
          name: k6-script
